{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from shapely.geometry import Polygon\n",
    "import folium\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "import h3\n",
    "import git\n",
    "\n",
    "from agent_components.llms.chatAI import ChatAIHandler\n",
    "from evaluation.candidate_generation_evaluation import CustomJSONEncoder\n",
    "import re\n"
   ],
   "id": "c668dc82a4c39a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\data\\processed_GeoCoDe_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    geocode_test_set = json.load(f)"
   ],
   "id": "745c031d63be3871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "geocode_test_set",
   "id": "160f5408d05ff10e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geo_relatable_harsh = [\n",
    "    article for article in geocode_test_set\n",
    "    if (any(word in article[\"article_text\"] for word in [\"lies\", \"located\"]) and\n",
    "        \" is a \" in article[\"article_text\"] and\n",
    "        any(word in article[\"article_text\"] for word in [\"km\", \"kilometer\", \"kilometre\"]))\n",
    "]\n",
    "print(len(geo_relatable_harsh))\n",
    "geo_relatable_harsh"
   ],
   "id": "58e89e539cf7480a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\n",
    "        fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\reflective_candidate_resolution\\fatal_error_and_invalid_correction\\GeoCoDe\\meta-llama-3.1-8b-instruct_with_meta-llama-3.1-8b-instruct_critic\\20250122_seed_24_1000_articles\\correct_articles_k_161.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    correctly_geocoded_articles = json.load(f)\n",
    "print(len(correctly_geocoded_articles))\n",
    "correctly_geocoded_articles"
   ],
   "id": "e9a251ed9a346216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geo_relatable_harsh_from_geocoded_articles = [\n",
    "    article for article in correctly_geocoded_articles\n",
    "    if (any(word in article[\"article_text\"] for word in [\"lies\", \"located\"]) and\n",
    "        any(word in article[\"article_text\"] for word in [\"km\", \"kilometer\", \"kilometre\"]))\n",
    "]\n",
    "print(len(geo_relatable_harsh_from_geocoded_articles))\n",
    "print(f\"# Toponyms: {sum(len(article['toponyms']) for article in geo_relatable_harsh_from_geocoded_articles)}\")\n",
    "geo_relatable_harsh_from_geocoded_articles"
   ],
   "id": "263f354e4cd2990a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run LLM GeoRelation for all fully correctly geocoded georelatable articles",
   "id": "df623665f73a12e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\n",
    "        fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\reflective_candidate_resolution\\fatal_error_and_invalid_correction\\GeoCoDe\\meta-llama-3.1-8b-instruct_with_meta-llama-3.1-8b-instruct_critic\\20250122_seed_24_1000_articles\\articles_with_at_least_one_correct_toponym_k_161.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    correctly_geocoded_articles = json.load(f)"
   ],
   "id": "f89f9d851b7e184c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def georelate(model_name, long_term_memory, article_text, mentioned_toponyms):\n",
    "    prompt = long_term_memory.generate_georelation_prompt(article_text=article_text,\n",
    "                                                          mentioned_toponyms=mentioned_toponyms,\n",
    "                                                          example_path=fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\data\\few_shot_example_georelation.json\")\n",
    "    handler = ChatAIHandler()\n",
    "    model = handler.get_model(model_name)\n",
    "    llm_answer = model.invoke(prompt)\n",
    "    pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "\n",
    "    thoughts = pattern.findall(llm_answer.content)  # Extract thoughts\n",
    "    non_thoughts = pattern.sub(\"\", llm_answer.content)  # Remove thoughts from text\n",
    "    start = non_thoughts.find(\"```json\")\n",
    "    if start != -1:\n",
    "        start = start + 7\n",
    "        end = non_thoughts.find(\"```\", start)  # Find the next occurrence after start\n",
    "        content = non_thoughts[start:end]\n",
    "        prediction = json.loads(content)\n",
    "    elif non_thoughts.find(\"```\") != -1:\n",
    "        start = start + 4\n",
    "        end = non_thoughts.find(\"```\", start)  # Find the next occurrence after start\n",
    "        content = non_thoughts[start:end]\n",
    "        prediction = json.loads(content)\n",
    "    else:\n",
    "        prediction = json.loads(llm_answer.content)\n",
    "    return thoughts, prediction"
   ],
   "id": "5df77d3880d1c1cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "model_name = \"llama-3.3-70b-instruct\" #\"deepseek-r1-distill-llama-70b\"\n",
    "directory = f'output/georelation/all_articles_with_at_lest_one_correctly_geocoded_toponym/{model_name}/{pd.Timestamp.now().strftime(\"%Y%m%d\")}'\n",
    "output_directory = os.path.join(r\"C:\\Users\\kaimo\\Uni\\Master\\5_WS24_25-MA\\geoparse-natural-disasters-with-llms\",\n",
    "                                directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "ltm = LongTermMemory(documentation_file=fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\agent_components\\memory\\external_tool_documentation\\geonames_websearch_documentation.md\")\n",
    "handler = ChatAIHandler()\n",
    "model = handler.get_model(model_name)\n",
    "for article in correctly_geocoded_articles:\n",
    "    if os.path.exists(f\"{output_directory}/{article['article_id']}.json\"):\n",
    "        continue\n",
    "    prediction = georelate(model, ltm, article['article_text'], article['correctly_geocoded_toponyms'])\n",
    "    if prediction:\n",
    "        article.update({\"georelation\": prediction})\n",
    "        # save article to json file in a specified directory\n",
    "        try:\n",
    "            with open(f\"{output_directory}/{article['article_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(article, f, ensure_ascii=False, indent=4)\n",
    "        except Exception as e:\n",
    "            article_ids = article[\"article_id\"].split()\n",
    "            last_id = article_ids[-1]  # Get last ID\n",
    "            with open(f\"{output_directory}/{last_id}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(article, f, ensure_ascii=False, indent=4)"
   ],
   "id": "f7ede19b9d2266ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add ground truth center coordinates, bounding box and approx area to articles",
   "id": "eed7c2e6115e23da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_bounding_box(relation_id):\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    relation({relation_id});\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://overpass-api.de/api/interpreter\"\n",
    "        response = requests.post(url, data={\"data\": query})\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['elements']:\n",
    "            return None, None, None, None\n",
    "\n",
    "        relation = data['elements'][0]\n",
    "        if 'bounds' not in relation:\n",
    "            return None, None, None, None\n",
    "\n",
    "        bounds = relation['bounds']\n",
    "        min_lat = bounds['minlat']\n",
    "        max_lat = bounds['maxlat']\n",
    "        min_lon = bounds['minlon']\n",
    "        max_lon = bounds['maxlon']\n",
    "\n",
    "        return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching relation {relation_id}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_osm_admin_center(osm_id):\n",
    "    \"\"\"\n",
    "    Query Overpass API for the admin_center or center of the given OSM ID.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    relation({osm_id});\n",
    "    out center;\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://overpass-api.de/api/interpreter\"\n",
    "    response = requests.get(url, params={\"data\": query})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        elements = data.get(\"elements\", [])\n",
    "\n",
    "        for element in elements:\n",
    "            if \"center\" in element:\n",
    "                return element[\"center\"]  # Returns {lat, lon}\n",
    "\n",
    "    return None  # Return None if no center/admin_center is found\n",
    "\n",
    "def get_area_sq_km(geometry, source_crs=\"EPSG:4326\", target_crs=\"EPSG:3857\"):\n",
    "    project = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True).transform\n",
    "    projected_geometry = transform(project, geometry)\n",
    "    area_sq_km = projected_geometry.area / 1_000_000\n",
    "    return area_sq_km\n",
    "\n",
    "\n",
    "def process_articles(articles):\n",
    "    \"\"\"\n",
    "    Extracts the last ID from article_id and fetches its coordinates.\n",
    "    \"\"\"\n",
    "    processed_articles = []\n",
    "    results = {}\n",
    "\n",
    "    for article in articles:\n",
    "        article_ids = article[\"article_id\"].split()\n",
    "        last_id = article_ids[-1]  # Get last ID\n",
    "\n",
    "        centroid_coords = get_osm_admin_center(last_id)\n",
    "\n",
    "        bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = get_bounding_box(last_id)\n",
    "        if all((bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon)):\n",
    "            bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                        (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "\n",
    "            bb_area = get_area_sq_km(bounding_box_polygon)\n",
    "            if centroid_coords and bb_area:\n",
    "                article.update({'gt': {'centroid': centroid_coords,\n",
    "                                       'bounding_box':{\n",
    "                                           'bb_min_lat': bb_min_lat,\n",
    "                                           'bb_max_lat': bb_max_lat,\n",
    "                                           'bb_min_lon': bb_min_lon,\n",
    "                                           'bb_max_lon': bb_max_lon\n",
    "                                       },\n",
    "                                       'bb area': bb_area}})\n",
    "                processed_articles.append(article)\n",
    "\n",
    "    return processed_articles"
   ],
   "id": "89afd2451e6d5966",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# articles_for_georelation = process_articles(correctly_geocoded_articles)",
   "id": "ec61c320aacde3ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\articles_for_georelation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles_for_georelation = json.load(f)\n",
    "len(articles_for_georelation)"
   ],
   "id": "a2086401eb808d45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# H3: Resolution handling",
   "id": "b28462dc274fc553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define H3 resolutions with their corresponding hexagon areas (from table)\n",
    "h3_data = [\n",
    "    (0, 4357449.416078381),\n",
    "    (1, 609788.441794133),\n",
    "    (2, 86801.780398997),\n",
    "    (3, 12393.434655088),\n",
    "    (4, 1770.347654491),\n",
    "    (5, 252.903858182),\n",
    "    (6, 36.129062164),\n",
    "    (7, 5.161293360),\n",
    "    (8, 0.737327598),\n",
    "    (9, 0.105332513),\n",
    "    (10, 0.015047502),\n",
    "    (11, 0.002149643),\n",
    "    (12, 0.000307092),\n",
    "    (13, 0.000043870),\n",
    "    (14, 0.000006267),\n",
    "    (15, 0.000000895),\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(h3_data, columns=[\"Resolution\", \"Hexagon_Area_km2\"])\n",
    "\n",
    "def get_h3_resolution_scaled(area_km2):\n",
    "    \"\"\"Find the H3 resolution where the input area and hexagon area are most proportionate.\"\"\"\n",
    "    df[\"Scale_Diff\"] = np.abs(np.log2(area_km2 / df[\"Hexagon_Area_km2\"]))\n",
    "    closest_resolution = df.loc[df[\"Scale_Diff\"].idxmin()]\n",
    "    return int(closest_resolution[\"Resolution\"])\n",
    "\n",
    "# # Example usage:\n",
    "# input_area = 100  # Change this to your area in km¬≤\n",
    "# best_resolution = get_h3_resolution_scaled(input_area)\n",
    "# print(f\"Best H3 resolution for {input_area} km¬≤: {best_resolution}\")"
   ],
   "id": "983790e582437e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parsing of Complex Location Descriptions Evaluation",
   "id": "5fffa928fdf84e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_json_file(directory: str, search_string: str):\n",
    "    \"\"\"\n",
    "    Loops over all files in the specified directory and loads JSON files\n",
    "    whose filenames contain the specified search string.\n",
    "\n",
    "    :param directory: Path to the directory containing files.\n",
    "    :param search_string: String to search for in filenames.\n",
    "    :return: A dictionary with filenames as keys and loaded JSON content as values.\n",
    "    \"\"\"\n",
    "    json_data = None\n",
    "\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(f\"The specified path '{directory}' is not a directory.\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if search_string in filename and filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                break\n",
    "    return json_data\n",
    "\n",
    "GEORELATION_DIR = fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\llama-3.3-70b-instruct\\20250206\"\n",
    "k = 161\n",
    "\n",
    "all_error_distances = []\n",
    "all_squared_area_diffs = []\n",
    "all_log_q = []\n",
    "all_normalized_area_diffs = []\n",
    "evaluation_results = []\n",
    "very_off_area_predictions = []\n",
    "\n",
    "for gt_article in articles_for_georelation:\n",
    "    eval_results_for_article = {}\n",
    "    article_ids = gt_article[\"article_id\"].split()\n",
    "    last_id = article_ids[-1]  # Get last ID\n",
    "\n",
    "    # first, load generation\n",
    "    try:\n",
    "        georelated_article = load_json_file(GEORELATION_DIR, gt_article[\"article_id\"])\n",
    "        if not georelated_article:\n",
    "            for id in article_ids:\n",
    "                georelated_article = load_json_file(GEORELATION_DIR, id)\n",
    "                if georelated_article:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    if not georelated_article:\n",
    "        continue\n",
    "\n",
    "    # calculate distance of coordinates\n",
    "    gt_coordinates = (gt_article['gt']['centroid']['lat'],\n",
    "                      gt_article['gt']['centroid']['lon'])\n",
    "    predicted_coordinates = (georelated_article['georelation']['coordinates of geographical unit']['latitude'],\n",
    "                             georelated_article['georelation']['coordinates of geographical unit']['longitude'])\n",
    "    error_distance = geodesic(gt_coordinates, predicted_coordinates).kilometers\n",
    "    all_error_distances.append(error_distance)\n",
    "\n",
    "    # calculate area error distance\n",
    "    gt_area = gt_article['gt']['bb area']\n",
    "    predicted_area = georelated_article[\"georelation\"][\"area in square km\"]\n",
    "\n",
    "    log_q = np.log(predicted_area/gt_area)\n",
    "    all_log_q.append(log_q)\n",
    "\n",
    "    squared_area_diff = (predicted_area - gt_area)**2\n",
    "    all_squared_area_diffs.append(squared_area_diff)\n",
    "\n",
    "    normalized_error_diff = (predicted_area - gt_area) / gt_area\n",
    "    if normalized_error_diff > 10:\n",
    "        very_off_area_predictions.append(georelated_article)\n",
    "    all_normalized_area_diffs.append(normalized_error_diff)\n",
    "\n",
    "    # calculate intersection\n",
    "    # Define an H3 cell\n",
    "    h3_index = h3.latlng_to_cell(\n",
    "        lat=georelated_article[\"georelation\"][\"coordinates of geographical unit\"][\"latitude\"],\n",
    "        lng=georelated_article[\"georelation\"][\"coordinates of geographical unit\"][\"longitude\"],\n",
    "        res=get_h3_resolution_scaled(predicted_area)\n",
    "    )\n",
    "\n",
    "    # Get the boundary coordinates of the H3 cell\n",
    "    hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "    # Convert polygon to shapely format\n",
    "    hexagon_coords = [(lng, lat) for lat, lng in hexagon]  # Convert to (lng, lat) for Folium\n",
    "    hexagon_polygon = Polygon(hexagon_coords)\n",
    "    hexagon_area_estimate = get_area_sq_km(hexagon_polygon)\n",
    "\n",
    "    bb = gt_article['gt']['bounding_box']\n",
    "    bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = bb['bb_min_lat'], bb['bb_max_lat'], bb['bb_min_lon'], bb['bb_max_lon']\n",
    "    bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                    (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "    intersection = bounding_box_polygon.intersection(hexagon_polygon)\n",
    "    if not intersection.is_empty:\n",
    "        intersection_area = get_area_sq_km(intersection)\n",
    "    else:\n",
    "        intersection_area = 0\n",
    "\n",
    "    eval_results_for_article.update({\"article_id\": last_id,\n",
    "                                     \"gt\": gt_article['gt'],\n",
    "                                     \"georelation\": georelated_article['georelation'],\n",
    "                                     \"absolute_error_distance\": error_distance,\n",
    "                                     \"squared_area_error\": squared_area_diff,\n",
    "                                     \"log_q\": log_q,\n",
    "                                     \"normalized_area_error\": normalized_error_diff,\n",
    "                                     \"hexagon_area_estimate\": hexagon_area_estimate,\n",
    "                                     \"area_intersection\": intersection_area})\n",
    "    evaluation_results.append(eval_results_for_article)\n",
    "\n",
    "all_normalized_area_diffs = np.array(all_normalized_area_diffs)\n",
    "# Accuracy@k\n",
    "within_k = [d <= k for d in all_error_distances]\n",
    "accuracy_at_k = sum(within_k) / len(all_error_distances)\n",
    "print(f\"Accuracy@{k}: {accuracy_at_k}\")\n",
    "\n",
    "# AUC\n",
    "def calculate_auc(sorted_values):\n",
    "    max_error = 20038  # Earth's circumference in km / 2 (maximum possible distance)\n",
    "    size = len(sorted_values)\n",
    "    if size <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    h = 1  # step size\n",
    "    sum = 0.5 * (np.log(1 + sorted_values[0]) / np.log(max_error) + np.log(\n",
    "        1 + sorted_values[-1]) / np.log(max_error))  # initial area\n",
    "\n",
    "    for i in range(1, size - 1):\n",
    "        sum += np.log(1 + sorted_values[i]) / np.log(max_error)\n",
    "\n",
    "    auc = sum * h / (size - 1)\n",
    "    return auc\n",
    "\n",
    "sorted_error_distances = sorted(\n",
    "    all_error_distances)  # assuming error_distances is a dictionary with error error_distances\n",
    "auc = calculate_auc(sorted_error_distances)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "# Mean error distance\n",
    "mean_error_distance = np.mean(all_error_distances)\n",
    "print(f\"Mean error distance: {mean_error_distance}\")\n",
    "\n",
    "# Median error distance\n",
    "median_error_distance = np.median(all_error_distances)\n",
    "print(f\"Median error distance: {median_error_distance}\")\n",
    "\n",
    "# ùúÅ Median Symmetric Accuracy\n",
    "abs_all_log_q = np.abs(all_log_q)\n",
    "all_log_q = np.array(all_log_q)\n",
    "zeta = 100 * (np.exp(np.median(abs_all_log_q)) - 1)\n",
    "print(f\"Median Symmetric Accuracy: {zeta}\")\n",
    "\n",
    "# SSPB\n",
    "sspb = 100 * np.sign(np.median(all_log_q)) * (np.exp(np.abs(np.median(all_log_q))) - 1)\n",
    "print(f\"Symmetric Signed Percentage Bias: {sspb}\")\n",
    "\n",
    "print(f\"nof very off area predictions: {len(very_off_area_predictions)}\")\n"
   ],
   "id": "725eb8a109ed0fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "average_precision = np.sum([evaluation[\"area_intersection\"]/evaluation['hexagon_area_estimate'] for evaluation in evaluation_results])/len(evaluation_results)\n",
    "average_recall = np.sum([evaluation[\"area_intersection\"]/evaluation['gt']['bb area'] for evaluation in evaluation_results])/len(evaluation_results)\n",
    "average_f1 = 2*average_precision*average_recall/(average_precision+average_recall)\n",
    "\n",
    "print(f\"average precision: {average_precision}, \\naverage recall: {average_recall}, \\naverage f1: {average_f1}\")"
   ],
   "id": "ebc5a29726be6539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, box\n",
    "import shapely.ops as ops\n",
    "\n",
    "def create_hexagon(side_length=1):\n",
    "    \"\"\"Create a regular hexagon centered at the origin with a given side length.\"\"\"\n",
    "    angles = np.linspace(0, 2 * np.pi, 7)[:-1]  # Six angles\n",
    "    vertices = [(side_length * np.cos(a), side_length * np.sin(a)) for a in angles]\n",
    "    return Polygon(vertices)\n",
    "\n",
    "def create_rectangle(width, height):\n",
    "    \"\"\"Create a rectangle centered at the origin with given width and height.\"\"\"\n",
    "    half_w, half_h = width / 2, height / 2\n",
    "    vertices = [(-half_w, -half_h), (half_w, -half_h), (half_w, half_h), (-half_w, half_h)]\n",
    "    return Polygon(vertices)\n",
    "\n",
    "def compute_f1(hexagon, rectangle):\n",
    "    \"\"\"Compute F1 score based on intersection, precision, and recall.\"\"\"\n",
    "    intersection = hexagon.intersection(rectangle).area\n",
    "    precision = intersection / hexagon.area\n",
    "    recall = intersection / rectangle.area if rectangle.area > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def simulate_f1():\n",
    "    \"\"\"Simulate the F1 score as the rectangle shrinks from the hexagon's bounding box and visualize with a heat map.\"\"\"\n",
    "    hexagon = create_hexagon()\n",
    "    bbox = hexagon.bounds  # (minx, miny, maxx, maxy)\n",
    "    max_width, max_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "\n",
    "    max_overall = max(max_height, max_width)\n",
    "\n",
    "    width_values = height_values = np.linspace(0.1, max_overall, 1000)\n",
    "\n",
    "    f1_matrix = np.zeros((len(height_values), len(width_values)))\n",
    "    best_f1, best_w, best_h = 0, 0, 0\n",
    "\n",
    "    for i, h in enumerate(height_values):\n",
    "        for j, w in enumerate(width_values):\n",
    "            rectangle = create_rectangle(w, h)\n",
    "            f1_score = compute_f1(hexagon, rectangle)\n",
    "            f1_matrix[i, j] = f1_score\n",
    "            if f1_score > best_f1:\n",
    "                best_f1, best_w, best_h = f1_score, w, h\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(f1_matrix, extent=[0.1, max_overall, 0.1, max_overall], origin='lower', aspect='auto', cmap='coolwarm')\n",
    "    plt.colorbar(label='F1 Score')\n",
    "    plt.xlabel('Rectangle Width')\n",
    "    plt.ylabel('Rectangle Height')\n",
    "    plt.title('F1 Score Heat Map for Hexagon-Rectangle Overlap')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the hexagon and the best rectangle\n",
    "    best_rectangle = create_rectangle(best_w, best_h)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    x, y = hexagon.exterior.xy\n",
    "    ax.plot(x, y, 'b-', label='Hexagon')\n",
    "\n",
    "    x, y = best_rectangle.exterior.xy\n",
    "    ax.plot(x, y, 'r-', label=f'Best Rectangle (w={best_w:.3f}, h={best_h:.3})')\n",
    "\n",
    "    ax.set_xlim(-max_overall / 2, max_overall / 2)\n",
    "    ax.set_ylim(-max_overall / 2, max_overall / 2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Best F1 Score Rectangle within Hexagon')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return f1_matrix, best_f1\n",
    "\n",
    "# f1_matrix, best_f1 = simulate_f1()"
   ],
   "id": "a2ebc99c39625269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# best_f1",
   "id": "b7d5ec88897149a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_map(bb, hexagon, intersection):\n",
    "    if not bb:\n",
    "        return None\n",
    "    lat_center, lon_center = (bb.centroid.y, bb.centroid.x)\n",
    "    m = folium.Map(location=(lat_center, lon_center), zoom_start=10)\n",
    "\n",
    "    folium.Polygon(locations=[(lat, lon) for lon, lat in bb.exterior.coords], color=\"yellow\", fill=True, fill_opacity=0.3, weight=2).add_to(m)\n",
    "\n",
    "    folium.Polygon(locations=[(lat, lon) for lon, lat in hexagon.exterior.coords], color=\"blue\", fill=True, fill_opacity=0.3, weight=2).add_to(m)\n",
    "\n",
    "    if intersection:\n",
    "        folium.Polygon(locations=[(lat, lon) for lon, lat in intersection.exterior.coords],\n",
    "                       color=\"green\", fill=True, fill_opacity=0.3, weight=2).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# ID = \"4482873 5924711\"\n",
    "osm_id = '108296'\n",
    "GEORELATION_DIR = fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\llama-3.3-70b-instruct\\20250206\"\n",
    "georelated_article = load_json_file(GEORELATION_DIR, osm_id)\n",
    "\n",
    "georelation_prediction = georelated_article['georelation']\n",
    "\n",
    "h3_index = h3.latlng_to_cell(\n",
    "    georelation_prediction[\"coordinates of geographical unit\"][\"latitude\"],\n",
    "    georelation_prediction[\"coordinates of geographical unit\"][\"longitude\"],\n",
    "    get_h3_resolution_scaled(georelation_prediction[\"area in square km\"])\n",
    ")\n",
    "\n",
    "hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "hexagon_coords = [(lon, lat) for lat, lon in hexagon]\n",
    "hexagon_polygon = Polygon(hexagon_coords)\n",
    "predicted_hexagon_area = get_area_sq_km(hexagon_polygon)\n",
    "print(f\"Predicted Hexagon area: {predicted_hexagon_area} square km\")\n",
    "\n",
    "bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = get_bounding_box(osm_id)\n",
    "if bb_min_lat is not None and bb_max_lat is not None and bb_min_lon is not None and bb_max_lon is not None:\n",
    "    bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                    (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "    area = get_area_sq_km(bounding_box_polygon)\n",
    "    print(f\"Bounding box area: {area} square km\")\n",
    "\n",
    "    intersection = bounding_box_polygon.intersection(hexagon_polygon)\n",
    "\n",
    "    if not intersection.is_empty:\n",
    "        intersection_area = get_area_sq_km(intersection)\n",
    "        print(f\"Intersection area: {intersection_area} square km\")\n",
    "\n",
    "    m = create_map(bounding_box_polygon, hexagon_polygon, intersection)\n",
    "else:\n",
    "    print(\"Could not retrieve bounding box\")\n",
    "\n",
    "m"
   ],
   "id": "f2c0a081168d5747",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(evaluation_results)",
   "id": "92f87382a956b740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GeoRelating",
   "id": "4c0e917132eab8c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner') # initialize neural pipeline"
   ],
   "id": "114864b2aca49051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# article_name = \"ahrtal_floods\"\n",
    "# article_text = \"At least 19 people have died and dozens are reported missing as record rainfall in western Germany caused rivers to burst their banks, swept away homes and inundated cellars. Police said four people had died and 70 were missing on Thursday around the wine-growing hub of Ahrweiler, in Rhineland-Palatinate state, after the Ahr River that flows into the Rhine burst its banks and brought down half a dozen houses. ‚ÄúWe have never seen such a catastrophe, it is truly devastating,‚Äù Rhineland-Palatinate premier Malu Dreyer told state lawmakers. To the north, in North Rhine-Westphalia, two people were found dead in flooded cellars in Cologne, with further deaths in Solingen, Unna and Rheinbach, police said. On Wednesday night, two firefighters died in the state ‚Äì one drowned while the other collapsed after a rescue mission. About 50 people were stranded on roofs in Ahrweiler, and more houses were at risk of collapse. Police helicopters flew in from neighbouring states to winch people to safety, Koblenz police said. Weather experts said rains in the region over the past 24 hours had been unprecedented, as a near-stationary low-pressure weather system caused sustained local precipitation also to the west in France and the Netherlands. More heavy rainfall is due in southwestern Germany, on the upper reaches of the German Rhine, later on Thursday and Friday, the German Weather Service said.\"\n",
    "# article_title = \"Death toll rises in ‚Äòdevastating‚Äô German floods\"\n",
    "# #https://www.aljazeera.com/gallery/2021/7/15/death-toll-rises-in-devastating-german-floods\n",
    "\n",
    "# article_name = \"central_europe_floods\"\n",
    "# article_text = \"Residents in several regions of Poland and the Czech Republic on Monday evacuated their homes as central Europe began recovering from the worst flooding in over 20 years, which has resulted in widespread damage and an increasing death toll. The border areas between Poland and the Czech Republic were heavily impacted over the weekend by torrential rains that began last week. Rising water levels led to the collapse of bridges, forced evacuations, and significant damage to vehicles and homes. Recent reports indicate that at least 17 fatalities have occurred due to the flooding stretching from Romania to Poland. On Monday afternoon, the mayor of Nysa, a town with over 40,000 residents in southern Poland, urged locals to evacuate immediately after a nearby floodbank was damaged. Industrial, residential areas hit hard In the northeastern Czech city of Ostrava, a breached barrier on the Odra River, where it meets the Opava River, resulted in flooding that affected the city‚Äôs industrial zone, including the BorsodChem chemical plant and the OKK Koksovny coking plant. Many residents were also being evacuated from nearby residential neighbourhoods. Meanwhile, in the Czech town of Litovel, where around 70 per cent of the area was submerged by water up to one metre deep (3.2 feet) on Monday, residents reported a swift and terrifying rise in water levels over the weekend.\"\n",
    "# article_title = \"Central Europe floods: Poland, Czech Republic evacuate amid 17 deaths\"\n",
    "# https://www.business-standard.com/world-news/central-europe-floods-poland-czech-republic-evacuate-amid-17-deaths-124091700590_1.html\n",
    "\n",
    "# article_name = \"zyklon_alfred\"\n",
    "# article_text = \"In Australien erwarten Millionen Menschen besorgt die Ankunft von Zyklon ‚ÄûAlfred‚Äú. Der Tropensturm bewegt sich weiter langsam, aber sicher auf eine dicht besiedelte und auch bei Touristen beliebte Region an der Ostk√ºste zu. J√ºngsten Berechnungen von Meteorologen zufolge wird er wahrscheinlich am Samstagmorgen (Ortszeit) auf Land treffen - sp√§ter als zun√§chst erwartet. Zuletzt befand sich ‚ÄûAlfred‚Äú noch etwa 100 Kilometer s√ºd√∂stlich der Metropole Brisbane, mit rund 2,5 Millionen Einwohnern die drittgr√∂√üte Stadt des Landes. Es ist der erste Zyklon seit 50 Jahren, der in der Region die K√ºste erreichen wird. Die meisten Schulen, Superm√§rkte, Stra√üen und Flugh√§fen wurden vorsorglich geschlossen. Die Beh√∂rden hatten Hunderttausende Sands√§cke verteilt, damit die Menschen ihre H√§user und Gesch√§fte sichern konnten. Auch f√ºr Tiere wie Pferde und gr√∂√üeres Vieh wurden Evakuierungszentren eingerichtet. Bereits die ersten Ausl√§ufer des Wirbelsturms rissen B√§ume und Strommasten um. Mehr als 80.000 Anwohner sind schon jetzt ohne Strom, wie der Sender ABC berichtete. Ein Mann wurde in der N√§he der Kleinstadt Dorrigo von den Wassermassen in einen Fluss gerissen - die Einsatzkr√§fte suchten nach ihm. Neun Meter hohe Wellen Vor der K√ºste t√ºrmten sich rund neun Meter hohe Wellen, wie Jane Golding vom staatlichen Wetterdienst betonte. Die Brandung sp√ºlte viele der bekannten Str√§nde fort. ‚ÄûWir erleben bereits Erosion an den Str√§nden der Gold Coast, die in dicht besiedelten Gebieten von Surfers Paradise und Main Beach bis hinunter nach Burleigh und Coolangatta Klippen von bis zu zwei Metern H√∂he verursacht hat‚Äú, sagte Natalie Edwards von Surf Life Saving Queensland. Zyklon ‚ÄûAlfred‚Äú n√§hert sich Australiens Ostk√ºste. Starkregen, B√∂en von bis zu 150 Kilometern pro Stunde und Monsterwellen werden vor allem in der Region zwischen der Sunshine Coast in Queensland und dem 300 Kilometer weiter s√ºdlich gelegenen Surfer-Hotspot Byron Bay in New South Wales erwartet. Die Beh√∂rden bef√ºrchten schwere √úberschwemmungen. ‚ÄûHeftige bis √∂rtlich extreme Regenf√§lle k√∂nnen zu gef√§hrlichen und lebensgef√§hrlichen Sturzfluten f√ºhren‚Äú, warnte der Wetterdienst. ‚ÄûIch rate allen, die in der Evakuierungszone leben oder sich dort aufhalten, dringend, das Gebiet jetzt zu verlassen‚Äú, sagte Polizeisprecher Scott Tanner. ‚ÄûDas Wetter wird sich in den n√§chsten Stunden verschlechtern, und dann k√∂nnte es zu sp√§t sein, um sich in Sicherheit zu bringen.‚Äú\"\n",
    "# article_title = \"Zyklon ‚ÄûAlfred‚Äú n√§hert sich Australiens Ostk√ºste\"\n",
    "#https://www.faz.net/aktuell/gesellschaft/ungluecke/zyklon-alfred-naehert-sich-australiens-ostkueste-110341897.html\n",
    "\n",
    "article_name = \"the_mountain_wildfire\"\n",
    "article_text = \"A fast-moving wildfire erupted in Southern California on Wednesday, destroying homes and sending firefighters rushing to get residents out of homes and to safety, officials said. \\nThe Mountain Fire in Ventura County prompted evacuation orders and grew to over 14,000 acres, fueled by what fire officials called a significant Santa Ana wind event. \\nFirefighters at the scene of the brush fire, which broke out between the communities of Moorpark and Somis, ‚Äúwere faced with a tough firefight,‚Äù Ventura County Fire Capt. Trevor Johnson said. \\n‚ÄúFirefighters were right off the bat engaged in pulling people out of their houses and saving lives,‚Äù Johnson said. \\nThe fire was moving so fast that firefighters drove residents out of the area in fire engines because of the danger, he said. \\nThe fire department did not have a count for the number of destroyed structures. The fire began at 8:51 a.m. local time, said the California Department of Forestry and Fire Protection, known as Cal Fire. It was 0% contained Wednesday afternoon, and what sparked it was under investigation.\"\n",
    "article_title = \"California wildfire fueled by high winds grows to over 14,000 acres and forces evacuations\"\n",
    "#https://www.nbcnews.com/news/us-news/california-mountain-fire-fueled-high-winds-evacuat-rcna179047\n",
    "\n",
    "# article_name = \"earthquakes_santorini\"\n",
    "# article_text = \"Die Erdbebenserie rund um die beliebte Ferieninsel Santorini dauert an. Die Menschen, die auf der Insel geblieben sind, wurden am Wochenende immer wieder in Angst versetzt und nachts aus dem Schlaf gerissen. Nach Angaben der Beh√∂rden hat die Erde seit dem 1. Februar Hunderte Male gebebt. Die Frequenz der Beben hat aber etwas nachgelassen. Die Seismologen geben aber keine Entwarnung. Es k√∂nnte doch noch ein gr√∂√üeres Beben geben, hei√üt es immer wieder seitens der Experten.\\nSchulen bleiben geschlossen ‚Äì viele Einwohner geflohen\\nDie Regierung hat beschlossen, dass die Schulen auf Santorini und den benachbarten Eilanden Ios, Anafi und Amorgos auch kommende Woche geschlossen bleiben. \\nDer gr√∂√üte Teil der Bev√∂lkerung - vor allem Frauen, Kinder und √§ltere Menschen - hat die Insel verlassen und sich auf dem Festland in Sicherheit gebracht. \\nAuf den Stra√üen trifft man Feuerwehrleute, Beamte des Zivilschutzes und der Polizei an, die aus anderen Regionen des Landes nach Santorini und den umliegenden Inseln gebracht worden sind - f√ºr den Fall, dass es zu einem schweren Erdbeben kommt. \\nDie Stadt Athen hat zahlreiche Holzh√§user in einem Sommerlager f√ºr die Menschen ge√∂ffnet, die kein Geld oder Verwandte haben, um eine neue Bleibe zu finden, bis das Ph√§nomen um Santorini endet. \\nGefahr durch Vulkanaktivit√§t?\\nUnklar bleibt weiterhin, inwiefern die Erdbebenserie, die sich haupts√§chlich zwischen den Inseln Santorini und Amorgos ereignet, die Vulkane der Region ‚Äûgeweckt‚Äú haben k√∂nnte - eine Frage, die die Einwohner besorgt. \\n1950 hatte eine Eruption des Vulkans von Santorini schwere Sch√§den angerichtet. Die Meinungen der Geologen und Seismologen gehen auseinander. Einige Experten schlie√üen eine Eruption aus, andere halten sie f√ºr m√∂glich.\\nAusharrende geben sich gelassen\\nViele Einwohner versuchen, sich Mut zu machen. Ein Mann aus Santorini sagte der Athener Zeitung ‚ÄûKathimerini‚Äú (Sonntag): ‚ÄûSantoriner zu sein bedeutet, auf einer Bombe zu leben und sich nicht darum zu k√ºmmern.‚Äú\"\n",
    "# article_title = \"Kein Ende der Beben um Santorini\"\n",
    "# dpa:250209-930-369654/1"
   ],
   "id": "f7c06af770d0bccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "doc = nlp(article_text) # run annotation over a sentence\n",
    "print(doc.entities)"
   ],
   "id": "5c40cb5300153225",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "toponym_list = [entity.text for entity in doc.entities if entity.type in [\"LOC\", \"GPE\"]]",
   "id": "b27c46fea89b3a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Initialize GeoCoder\n",
    "\"\"\"\n",
    "from helpers.helpers import preprocess_data\n",
    "from models.candidates import CandidateGenerationState, GeoCodingState\n",
    "import time\n",
    "from modules.reflective_geocoding import ReflectiveGeoCoder\n",
    "\n",
    "call_times = []\n",
    "actor = \"llama-3.3-70b-instruct\"  # [\"deepseek-r1-distill-llama-70b\", \"meta-llama-3.1-8b-instruct\", \"llama-3.3-70b-instruct\", \"mistral-large-instruct\"]\n",
    "critic = \"mistral-large-instruct\"\n",
    "dataset = \"New\"\n",
    "\n",
    "geocoder = ReflectiveGeoCoder(\n",
    "    actor_model_name=actor,\n",
    "    critic_model_name=critic,\n",
    "    call_times=call_times,\n",
    "    skip_few_shot_loader=False,\n",
    "    data_set=dataset\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Reflective Candidate Generation\n",
    "\"\"\"\n",
    "generation_graph_builder = geocoder.build_graph()\n",
    "generation_agent_graph = generation_graph_builder.compile()"
   ],
   "id": "1229393a13c5351f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "input_state = {\n",
    "        \"article_id\": \"002\",\n",
    "        \"article_title\": article_title,\n",
    "        \"article_text\": article_text,\n",
    "        \"toponyms\": toponym_list\n",
    "}\n",
    "\n",
    "# Invoke the graph\n",
    "generation_agent_graph_answer = generation_agent_graph.invoke(input_state)\n",
    "candidate_generation = CandidateGenerationState(**generation_agent_graph_answer)\n",
    "print(f\"generation with {actor}_with_{critic}_critic : time taken: {time.time() - start} seconds.\")"
   ],
   "id": "bc4bed9300a53f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Run reflective candidate resolution graph\n",
    "\"\"\"\n",
    "candidate_generation_dict = preprocess_data(candidate_generation.model_dump(), GeoCodingState)\n",
    "resolution_graph_builder = geocoder.build_resolution_graph()\n",
    "resolution_agent_graph = resolution_graph_builder.compile()\n",
    "start = time.time()\n",
    "resolution_agent_graph_answer = resolution_agent_graph.invoke(candidate_generation_dict)\n",
    "candidate_resolution = GeoCodingState(**resolution_agent_graph_answer)\n",
    "print(f\"resolution with {actor}_with_{critic}_critic for: time taken: {time.time() - start} seconds.\\n\")"
   ],
   "id": "fb6b29bc6dcefcd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "GeoRelation\n",
    "\"\"\"\n",
    "from models.candidates import GeoCodingState\n",
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\georelation\\georelating\\the_mountain_fire_geocoding.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    candidate_resolution = json.load(f)\n",
    "    candidate_resolution = GeoCodingState(**candidate_resolution)\n",
    "geocoded_toponyms = []\n",
    "for topo in candidate_resolution.valid_geocoded_toponyms:\n",
    "    for item in candidate_resolution.toponyms_with_candidates:\n",
    "        if item.toponym_with_search_arguments.toponym.casefold() == topo.toponym.casefold():\n",
    "            # get the candidate with the correct geonameid\n",
    "            incorrect_geonameid = True\n",
    "            for candidate in item.candidates:\n",
    "                if topo.selected_candidate_geonameId == candidate[\"geonameId\"]:\n",
    "                    topo.coordinates = {\"latitude\": candidate[\"lat\"],\n",
    "                                        \"longitude\": candidate[\"lng\"]}\n",
    "                    geocoded_toponyms.append(topo)\n",
    "                    break\n",
    "            break\n",
    "geocoded_toponyms = [toponym.model_dump() for toponym in geocoded_toponyms]"
   ],
   "id": "150eaf5e24902d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "thoughts, prediction = georelate(\"deepseek-r1-distill-llama-70b\", geocoder.working_memory.long_term_memory, article_text, geocoded_toponyms)",
   "id": "a5826b29dc82b1b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}Àô\\output\\georelation\\georelating\\the_mountain_fire_georelation_llama_distill_R1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    output = json.load(f)\n",
    "thoughts = output['georelation_thoughts']\n",
    "prediction = output['georelation']\n",
    "print(thoughts)\n",
    "print(json.dumps(prediction, indent = 4))"
   ],
   "id": "af2027a20268c532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'output' in prediction:\n",
    "    prediction = prediction['output']\n",
    "prediction_to_save = candidate_resolution.model_dump()\n",
    "prediction_to_save[\"georelation\"] = prediction\n",
    "prediction_to_save[\"georelation_thoughts\"] = thoughts\n",
    "with open(f\"{article_name}_llama.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(prediction_to_save, f, ensure_ascii=False, indent=4, cls=CustomJSONEncoder)"
   ],
   "id": "2297dd75db36d680",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "h3_res = get_h3_resolution_scaled(prediction[\"area in square km\"])\n",
    "h3_index = h3.latlng_to_cell(\n",
    "    prediction[\"coordinates of geographical unit\"][\"latitude\"],\n",
    "    prediction[\"coordinates of geographical unit\"][\"longitude\"],\n",
    "    h3_res\n",
    ")\n",
    "\n",
    "hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "hexagon_coords = [(lon, lat) for lat, lon in hexagon]\n",
    "hexagon_polygon = Polygon(hexagon_coords)\n",
    "predicted_hexagon_area = get_area_sq_km(hexagon_polygon)\n",
    "lat_center, lon_center = (prediction[\"coordinates of geographical unit\"][\"latitude\"], prediction[\"coordinates of geographical unit\"][\"longitude\"])\n",
    "hex_popup_text = f\"Selected H3 cell: <b>{h3_index}</b><br>Predicted area: <b>{prediction[\"area in square km\"]} km¬≤</b>\"\n",
    "hex_popup_html = f'<div style=\"font-size: 30px;\">{hex_popup_text}</div>'\n",
    "hex_popup = folium.Popup(\n",
    "    hex_popup_html,\n",
    "    max_width=450,\n",
    "    show=True,\n",
    "    sticky=True,\n",
    "    style={\"font-size\": \"30px\", \"color\": \"blue\"}\n",
    ")\n",
    "m = folium.Map(location=(lat_center, lon_center), zoom_start=10)\n",
    "folium.Polygon(locations=[(lat, lon) for lon, lat in hexagon_polygon.exterior.coords], color=\"red\", fill=True, fill_opacity=0.3, weight=2, popup=hex_popup, tooltip=h3_index).add_to(m)\n",
    "for loc in geocoded_toponyms:\n",
    "    popup = folium.Popup(\n",
    "        f'<div style=\"font-size: 24px;\"><b>{loc['toponym']}</b><br>GeoNames ID: {loc['selected_candidate_geonameId']}</div>',\n",
    "        max_width=350,\n",
    "        show=True,\n",
    "        sticky=True\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=[loc['coordinates']['latitude'], loc['coordinates']['longitude']],\n",
    "        popup=popup,\n",
    "        tooltip=loc['toponym']\n",
    "    ).add_to(m)\n",
    "m.save(f\"{article_name}_distill.html\")\n",
    "m"
   ],
   "id": "72ab1597a3c85d01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "import os\n",
    "import folium\n",
    "import json\n",
    "\n",
    "base_url = \"http://api.geonames.org/search?\"\n",
    "\n",
    "params = {\"name_equals\": \"Moorpark\",\n",
    "          \"username\": \"KaiMo\",\n",
    "          \"type\": \"json\",\n",
    "          \"style\": \"full\"}\n",
    "url = base_url + urllib.parse.urlencode(params)\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    response = requests.get(url) #retry once\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error in GeoNamesAPI.search: {response.text}\")\n",
    "all_moorparks = response.json()\n",
    "moorparks_map = folium.Map()\n",
    "for loc in all_moorparks['geonames']:\n",
    "    folium.Marker(\n",
    "        location=[loc['lat'], loc['lng']],\n",
    "        popup=loc['name'],\n",
    "        tooltip=loc['name']\n",
    "    ).add_to(moorparks_map)\n",
    "moorparks_map.save(\"all_moorparks_map.html\")\n",
    "print(json.dumps(all_moorparks, indent=4))\n",
    "moorparks_map"
   ],
   "id": "952b5e808c024e43",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
