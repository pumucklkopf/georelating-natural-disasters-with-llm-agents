{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from shapely.geometry import Polygon\n",
    "import folium\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "import h3\n",
    "import git\n",
    "\n",
    "from agent_components.llms.chatAI import ChatAIHandler\n",
    "from evaluation.candidate_generation_evaluation import CustomJSONEncoder\n",
    "import re\n"
   ],
   "id": "c668dc82a4c39a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\data\\processed_GeoCoDe_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    geocode_test_set = json.load(f)"
   ],
   "id": "745c031d63be3871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "geocode_test_set",
   "id": "160f5408d05ff10e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geo_relatable_harsh = [\n",
    "    article for article in geocode_test_set\n",
    "    if (any(word in article[\"article_text\"] for word in [\"lies\", \"located\"]) and\n",
    "        \" is a \" in article[\"article_text\"] and\n",
    "        any(word in article[\"article_text\"] for word in [\"km\", \"kilometer\", \"kilometre\"]))\n",
    "]\n",
    "print(len(geo_relatable_harsh))\n",
    "geo_relatable_harsh"
   ],
   "id": "58e89e539cf7480a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\n",
    "        fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\reflective_candidate_resolution\\fatal_error_and_invalid_correction\\GeoCoDe\\meta-llama-3.1-8b-instruct_with_meta-llama-3.1-8b-instruct_critic\\20250122_seed_24_1000_articles\\correct_articles_k_161.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    correctly_geocoded_articles = json.load(f)\n",
    "print(len(correctly_geocoded_articles))\n",
    "correctly_geocoded_articles"
   ],
   "id": "e9a251ed9a346216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geo_relatable_harsh_from_geocoded_articles = [\n",
    "    article for article in correctly_geocoded_articles\n",
    "    if (any(word in article[\"article_text\"] for word in [\"lies\", \"located\"]) and\n",
    "        any(word in article[\"article_text\"] for word in [\"km\", \"kilometer\", \"kilometre\"]))\n",
    "]\n",
    "print(len(geo_relatable_harsh_from_geocoded_articles))\n",
    "print(f\"# Toponyms: {sum(len(article['toponyms']) for article in geo_relatable_harsh_from_geocoded_articles)}\")\n",
    "geo_relatable_harsh_from_geocoded_articles"
   ],
   "id": "263f354e4cd2990a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run LLM GeoRelation for all fully correctly geocoded georelatable articles",
   "id": "df623665f73a12e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\n",
    "        fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\reflective_candidate_resolution\\fatal_error_and_invalid_correction\\GeoCoDe\\meta-llama-3.1-8b-instruct_with_meta-llama-3.1-8b-instruct_critic\\20250122_seed_24_1000_articles\\articles_with_at_least_one_correct_toponym_k_161.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    correctly_geocoded_articles = json.load(f)"
   ],
   "id": "f89f9d851b7e184c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def georelate(model_name, long_term_memory, article_text, mentioned_toponyms):\n",
    "    prompt = long_term_memory.generate_georelation_prompt(article_text=article_text,\n",
    "                                                          mentioned_toponyms=mentioned_toponyms,\n",
    "                                                          example_path=fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\data\\few_shot_example_georelation.json\")\n",
    "    handler = ChatAIHandler()\n",
    "    model = handler.get_model(model_name)\n",
    "    llm_answer = model.invoke(prompt)\n",
    "    pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "\n",
    "    thoughts = pattern.findall(llm_answer.content)  # Extract thoughts\n",
    "    non_thoughts = pattern.sub(\"\", llm_answer.content)  # Remove thoughts from text\n",
    "    start = non_thoughts.find(\"```json\")\n",
    "    if start != -1:\n",
    "        start = start + 7\n",
    "        end = non_thoughts.find(\"```\", start)  # Find the next occurrence after start\n",
    "        content = non_thoughts[start:end]\n",
    "        prediction = json.loads(content)\n",
    "    elif non_thoughts.find(\"```\") != -1:\n",
    "        start = start + 4\n",
    "        end = non_thoughts.find(\"```\", start)  # Find the next occurrence after start\n",
    "        content = non_thoughts[start:end]\n",
    "        prediction = json.loads(content)\n",
    "    else:\n",
    "        prediction = json.loads(llm_answer.content)\n",
    "    return thoughts, prediction"
   ],
   "id": "5df77d3880d1c1cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "model_name = \"llama-3.3-70b-instruct\" #\"deepseek-r1-distill-llama-70b\"\n",
    "directory = f'output/georelation/all_articles_with_at_lest_one_correctly_geocoded_toponym/{model_name}/{pd.Timestamp.now().strftime(\"%Y%m%d\")}'\n",
    "output_directory = os.path.join(r\"C:\\Users\\kaimo\\Uni\\Master\\5_WS24_25-MA\\geoparse-natural-disasters-with-llms\",\n",
    "                                directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "ltm = LongTermMemory(documentation_file=fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\agent_components\\memory\\external_tool_documentation\\geonames_websearch_documentation.md\")\n",
    "handler = ChatAIHandler()\n",
    "model = handler.get_model(model_name)\n",
    "for article in correctly_geocoded_articles:\n",
    "    if os.path.exists(f\"{output_directory}/{article['article_id']}.json\"):\n",
    "        continue\n",
    "    prediction = georelate(model, ltm, article['article_text'], article['correctly_geocoded_toponyms'])\n",
    "    if prediction:\n",
    "        article.update({\"georelation\": prediction})\n",
    "        # save article to json file in a specified directory\n",
    "        try:\n",
    "            with open(f\"{output_directory}/{article['article_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(article, f, ensure_ascii=False, indent=4)\n",
    "        except Exception as e:\n",
    "            article_ids = article[\"article_id\"].split()\n",
    "            last_id = article_ids[-1]  # Get last ID\n",
    "            with open(f\"{output_directory}/{last_id}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(article, f, ensure_ascii=False, indent=4)"
   ],
   "id": "f7ede19b9d2266ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add ground truth center coordinates, bounding box and approx area to articles",
   "id": "eed7c2e6115e23da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_bounding_box(relation_id):\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    relation({relation_id});\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://overpass-api.de/api/interpreter\"\n",
    "        response = requests.post(url, data={\"data\": query})\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['elements']:\n",
    "            return None, None, None, None\n",
    "\n",
    "        relation = data['elements'][0]\n",
    "        if 'bounds' not in relation:\n",
    "            return None, None, None, None\n",
    "\n",
    "        bounds = relation['bounds']\n",
    "        min_lat = bounds['minlat']\n",
    "        max_lat = bounds['maxlat']\n",
    "        min_lon = bounds['minlon']\n",
    "        max_lon = bounds['maxlon']\n",
    "\n",
    "        return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching relation {relation_id}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_osm_admin_center(osm_id):\n",
    "    \"\"\"\n",
    "    Query Overpass API for the admin_center or center of the given OSM ID.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    relation({osm_id});\n",
    "    out center;\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://overpass-api.de/api/interpreter\"\n",
    "    response = requests.get(url, params={\"data\": query})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        elements = data.get(\"elements\", [])\n",
    "\n",
    "        for element in elements:\n",
    "            if \"center\" in element:\n",
    "                return element[\"center\"]  # Returns {lat, lon}\n",
    "\n",
    "    return None  # Return None if no center/admin_center is found\n",
    "\n",
    "def get_area_sq_km(geometry, source_crs=\"EPSG:4326\", target_crs=\"EPSG:3857\"):\n",
    "    project = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True).transform\n",
    "    projected_geometry = transform(project, geometry)\n",
    "    area_sq_km = projected_geometry.area / 1_000_000\n",
    "    return area_sq_km\n",
    "\n",
    "\n",
    "def process_articles(articles):\n",
    "    \"\"\"\n",
    "    Extracts the last ID from article_id and fetches its coordinates.\n",
    "    \"\"\"\n",
    "    processed_articles = []\n",
    "    results = {}\n",
    "\n",
    "    for article in articles:\n",
    "        article_ids = article[\"article_id\"].split()\n",
    "        last_id = article_ids[-1]  # Get last ID\n",
    "\n",
    "        centroid_coords = get_osm_admin_center(last_id)\n",
    "\n",
    "        bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = get_bounding_box(last_id)\n",
    "        if all((bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon)):\n",
    "            bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                        (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "\n",
    "            bb_area = get_area_sq_km(bounding_box_polygon)\n",
    "            if centroid_coords and bb_area:\n",
    "                article.update({'gt': {'centroid': centroid_coords,\n",
    "                                       'bounding_box':{\n",
    "                                           'bb_min_lat': bb_min_lat,\n",
    "                                           'bb_max_lat': bb_max_lat,\n",
    "                                           'bb_min_lon': bb_min_lon,\n",
    "                                           'bb_max_lon': bb_max_lon\n",
    "                                       },\n",
    "                                       'bb area': bb_area}})\n",
    "                processed_articles.append(article)\n",
    "\n",
    "    return processed_articles"
   ],
   "id": "89afd2451e6d5966",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# articles_for_georelation = process_articles(correctly_geocoded_articles)",
   "id": "ec61c320aacde3ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\articles_for_georelation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles_for_georelation = json.load(f)\n",
    "len(articles_for_georelation)"
   ],
   "id": "a2086401eb808d45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# H3: Resolution handling",
   "id": "b28462dc274fc553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define H3 resolutions with their corresponding hexagon areas (from table)\n",
    "h3_data = [\n",
    "    (0, 4357449.416078381),\n",
    "    (1, 609788.441794133),\n",
    "    (2, 86801.780398997),\n",
    "    (3, 12393.434655088),\n",
    "    (4, 1770.347654491),\n",
    "    (5, 252.903858182),\n",
    "    (6, 36.129062164),\n",
    "    (7, 5.161293360),\n",
    "    (8, 0.737327598),\n",
    "    (9, 0.105332513),\n",
    "    (10, 0.015047502),\n",
    "    (11, 0.002149643),\n",
    "    (12, 0.000307092),\n",
    "    (13, 0.000043870),\n",
    "    (14, 0.000006267),\n",
    "    (15, 0.000000895),\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(h3_data, columns=[\"Resolution\", \"Hexagon_Area_km2\"])\n",
    "\n",
    "def get_h3_resolution_scaled(area_km2):\n",
    "    \"\"\"Find the H3 resolution where the input area and hexagon area are most proportionate.\"\"\"\n",
    "    df[\"Scale_Diff\"] = np.abs(np.log2(area_km2 / df[\"Hexagon_Area_km2\"]))\n",
    "    closest_resolution = df.loc[df[\"Scale_Diff\"].idxmin()]\n",
    "    return int(closest_resolution[\"Resolution\"])\n",
    "\n",
    "# # Example usage:\n",
    "# input_area = 100  # Change this to your area in km²\n",
    "# best_resolution = get_h3_resolution_scaled(input_area)\n",
    "# print(f\"Best H3 resolution for {input_area} km²: {best_resolution}\")"
   ],
   "id": "983790e582437e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parsing of Complex Location Descriptions Evaluation",
   "id": "5fffa928fdf84e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_json_file(directory: str, search_string: str):\n",
    "    \"\"\"\n",
    "    Loops over all files in the specified directory and loads JSON files\n",
    "    whose filenames contain the specified search string.\n",
    "\n",
    "    :param directory: Path to the directory containing files.\n",
    "    :param search_string: String to search for in filenames.\n",
    "    :return: A dictionary with filenames as keys and loaded JSON content as values.\n",
    "    \"\"\"\n",
    "    json_data = None\n",
    "\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(f\"The specified path '{directory}' is not a directory.\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if search_string in filename and filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                break\n",
    "    return json_data\n",
    "\n",
    "GEORELATION_DIR = fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\llama-3.3-70b-instruct\\20250206\"\n",
    "k = 161\n",
    "\n",
    "all_error_distances = []\n",
    "all_squared_area_diffs = []\n",
    "all_log_q = []\n",
    "all_normalized_area_diffs = []\n",
    "evaluation_results = []\n",
    "very_off_area_predictions = []\n",
    "\n",
    "for gt_article in articles_for_georelation:\n",
    "    eval_results_for_article = {}\n",
    "    article_ids = gt_article[\"article_id\"].split()\n",
    "    last_id = article_ids[-1]  # Get last ID\n",
    "\n",
    "    # first, load generation\n",
    "    try:\n",
    "        georelated_article = load_json_file(GEORELATION_DIR, gt_article[\"article_id\"])\n",
    "        if not georelated_article:\n",
    "            for id in article_ids:\n",
    "                georelated_article = load_json_file(GEORELATION_DIR, id)\n",
    "                if georelated_article:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    if not georelated_article:\n",
    "        continue\n",
    "\n",
    "    # calculate distance of coordinates\n",
    "    gt_coordinates = (gt_article['gt']['centroid']['lat'],\n",
    "                      gt_article['gt']['centroid']['lon'])\n",
    "    predicted_coordinates = (georelated_article['georelation']['coordinates of geographical unit']['latitude'],\n",
    "                             georelated_article['georelation']['coordinates of geographical unit']['longitude'])\n",
    "    error_distance = geodesic(gt_coordinates, predicted_coordinates).kilometers\n",
    "    all_error_distances.append(error_distance)\n",
    "\n",
    "    # calculate area error distance\n",
    "    gt_area = gt_article['gt']['bb area']\n",
    "    predicted_area = georelated_article[\"georelation\"][\"area in square km\"]\n",
    "\n",
    "    log_q = np.log(predicted_area/gt_area)\n",
    "    all_log_q.append(log_q)\n",
    "\n",
    "    squared_area_diff = (predicted_area - gt_area)**2\n",
    "    all_squared_area_diffs.append(squared_area_diff)\n",
    "\n",
    "    normalized_error_diff = (predicted_area - gt_area) / gt_area\n",
    "    if normalized_error_diff > 10:\n",
    "        very_off_area_predictions.append(georelated_article)\n",
    "    all_normalized_area_diffs.append(normalized_error_diff)\n",
    "\n",
    "    # calculate intersection\n",
    "    # Define an H3 cell\n",
    "    h3_index = h3.latlng_to_cell(\n",
    "        lat=georelated_article[\"georelation\"][\"coordinates of geographical unit\"][\"latitude\"],\n",
    "        lng=georelated_article[\"georelation\"][\"coordinates of geographical unit\"][\"longitude\"],\n",
    "        res=get_h3_resolution_scaled(predicted_area)\n",
    "    )\n",
    "\n",
    "    # Get the boundary coordinates of the H3 cell\n",
    "    hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "    # Convert polygon to shapely format\n",
    "    hexagon_coords = [(lng, lat) for lat, lng in hexagon]  # Convert to (lng, lat) for Folium\n",
    "    hexagon_polygon = Polygon(hexagon_coords)\n",
    "    hexagon_area_estimate = get_area_sq_km(hexagon_polygon)\n",
    "\n",
    "    bb = gt_article['gt']['bounding_box']\n",
    "    bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = bb['bb_min_lat'], bb['bb_max_lat'], bb['bb_min_lon'], bb['bb_max_lon']\n",
    "    bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                    (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "    intersection = bounding_box_polygon.intersection(hexagon_polygon)\n",
    "    if not intersection.is_empty:\n",
    "        intersection_area = get_area_sq_km(intersection)\n",
    "    else:\n",
    "        intersection_area = 0\n",
    "\n",
    "    eval_results_for_article.update({\"article_id\": last_id,\n",
    "                                     \"gt\": gt_article['gt'],\n",
    "                                     \"georelation\": georelated_article['georelation'],\n",
    "                                     \"absolute_error_distance\": error_distance,\n",
    "                                     \"squared_area_error\": squared_area_diff,\n",
    "                                     \"log_q\": log_q,\n",
    "                                     \"normalized_area_error\": normalized_error_diff,\n",
    "                                     \"hexagon_area_estimate\": hexagon_area_estimate,\n",
    "                                     \"area_intersection\": intersection_area})\n",
    "    evaluation_results.append(eval_results_for_article)\n",
    "\n",
    "all_normalized_area_diffs = np.array(all_normalized_area_diffs)\n",
    "# Accuracy@k\n",
    "within_k = [d <= k for d in all_error_distances]\n",
    "accuracy_at_k = sum(within_k) / len(all_error_distances)\n",
    "print(f\"Accuracy@{k}: {accuracy_at_k}\")\n",
    "\n",
    "# AUC\n",
    "def calculate_auc(sorted_values):\n",
    "    max_error = 20038  # Earth's circumference in km / 2 (maximum possible distance)\n",
    "    size = len(sorted_values)\n",
    "    if size <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    h = 1  # step size\n",
    "    sum = 0.5 * (np.log(1 + sorted_values[0]) / np.log(max_error) + np.log(\n",
    "        1 + sorted_values[-1]) / np.log(max_error))  # initial area\n",
    "\n",
    "    for i in range(1, size - 1):\n",
    "        sum += np.log(1 + sorted_values[i]) / np.log(max_error)\n",
    "\n",
    "    auc = sum * h / (size - 1)\n",
    "    return auc\n",
    "\n",
    "sorted_error_distances = sorted(\n",
    "    all_error_distances)  # assuming error_distances is a dictionary with error error_distances\n",
    "auc = calculate_auc(sorted_error_distances)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "# Mean error distance\n",
    "mean_error_distance = np.mean(all_error_distances)\n",
    "print(f\"Mean error distance: {mean_error_distance}\")\n",
    "\n",
    "# Median error distance\n",
    "median_error_distance = np.median(all_error_distances)\n",
    "print(f\"Median error distance: {median_error_distance}\")\n",
    "\n",
    "# 𝜁 Median Symmetric Accuracy\n",
    "abs_all_log_q = np.abs(all_log_q)\n",
    "all_log_q = np.array(all_log_q)\n",
    "zeta = 100 * (np.exp(np.median(abs_all_log_q)) - 1)\n",
    "print(f\"Median Symmetric Accuracy: {zeta}\")\n",
    "\n",
    "# SSPB\n",
    "sspb = 100 * np.sign(np.median(all_log_q)) * (np.exp(np.abs(np.median(all_log_q))) - 1)\n",
    "print(f\"Symmetric Signed Percentage Bias: {sspb}\")\n",
    "\n",
    "print(f\"nof very off area predictions: {len(very_off_area_predictions)}\")\n"
   ],
   "id": "725eb8a109ed0fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "average_precision = np.sum([evaluation[\"area_intersection\"]/evaluation['hexagon_area_estimate'] for evaluation in evaluation_results])/len(evaluation_results)\n",
    "average_recall = np.sum([evaluation[\"area_intersection\"]/evaluation['gt']['bb area'] for evaluation in evaluation_results])/len(evaluation_results)\n",
    "average_f1 = 2*average_precision*average_recall/(average_precision+average_recall)\n",
    "\n",
    "print(f\"average precision: {average_precision}, \\naverage recall: {average_recall}, \\naverage f1: {average_f1}\")"
   ],
   "id": "ebc5a29726be6539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, box\n",
    "import shapely.ops as ops\n",
    "\n",
    "def create_hexagon(side_length=1):\n",
    "    \"\"\"Create a regular hexagon centered at the origin with a given side length.\"\"\"\n",
    "    angles = np.linspace(0, 2 * np.pi, 7)[:-1]  # Six angles\n",
    "    vertices = [(side_length * np.cos(a), side_length * np.sin(a)) for a in angles]\n",
    "    return Polygon(vertices)\n",
    "\n",
    "def create_rectangle(width, height):\n",
    "    \"\"\"Create a rectangle centered at the origin with given width and height.\"\"\"\n",
    "    half_w, half_h = width / 2, height / 2\n",
    "    vertices = [(-half_w, -half_h), (half_w, -half_h), (half_w, half_h), (-half_w, half_h)]\n",
    "    return Polygon(vertices)\n",
    "\n",
    "def compute_f1(hexagon, rectangle):\n",
    "    \"\"\"Compute F1 score based on intersection, precision, and recall.\"\"\"\n",
    "    intersection = hexagon.intersection(rectangle).area\n",
    "    precision = intersection / hexagon.area\n",
    "    recall = intersection / rectangle.area if rectangle.area > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def simulate_f1():\n",
    "    \"\"\"Simulate the F1 score as the rectangle shrinks from the hexagon's bounding box and visualize with a heat map.\"\"\"\n",
    "    hexagon = create_hexagon()\n",
    "    bbox = hexagon.bounds  # (minx, miny, maxx, maxy)\n",
    "    max_width, max_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "\n",
    "    max_overall = max(max_height, max_width)\n",
    "\n",
    "    width_values = height_values = np.linspace(0.1, max_overall, 1000)\n",
    "\n",
    "    f1_matrix = np.zeros((len(height_values), len(width_values)))\n",
    "    best_f1, best_w, best_h = 0, 0, 0\n",
    "\n",
    "    for i, h in enumerate(height_values):\n",
    "        for j, w in enumerate(width_values):\n",
    "            rectangle = create_rectangle(w, h)\n",
    "            f1_score = compute_f1(hexagon, rectangle)\n",
    "            f1_matrix[i, j] = f1_score\n",
    "            if f1_score > best_f1:\n",
    "                best_f1, best_w, best_h = f1_score, w, h\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(f1_matrix, extent=[0.1, max_overall, 0.1, max_overall], origin='lower', aspect='auto', cmap='coolwarm')\n",
    "    plt.colorbar(label='F1 Score')\n",
    "    plt.xlabel('Rectangle Width')\n",
    "    plt.ylabel('Rectangle Height')\n",
    "    plt.title('F1 Score Heat Map for Hexagon-Rectangle Overlap')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the hexagon and the best rectangle\n",
    "    best_rectangle = create_rectangle(best_w, best_h)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    x, y = hexagon.exterior.xy\n",
    "    ax.plot(x, y, 'b-', label='Hexagon')\n",
    "\n",
    "    x, y = best_rectangle.exterior.xy\n",
    "    ax.plot(x, y, 'r-', label=f'Best Rectangle (w={best_w:.3f}, h={best_h:.3})')\n",
    "\n",
    "    ax.set_xlim(-max_overall / 2, max_overall / 2)\n",
    "    ax.set_ylim(-max_overall / 2, max_overall / 2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Best F1 Score Rectangle within Hexagon')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return f1_matrix, best_f1\n",
    "\n",
    "# f1_matrix, best_f1 = simulate_f1()"
   ],
   "id": "a2ebc99c39625269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# best_f1",
   "id": "b7d5ec88897149a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_map(bb, hexagon, intersection):\n",
    "    if not bb:\n",
    "        return None\n",
    "    lat_center, lon_center = (bb.centroid.y, bb.centroid.x)\n",
    "    m = folium.Map(location=(lat_center, lon_center), zoom_start=10)\n",
    "\n",
    "    folium.Polygon(locations=[(lat, lon) for lon, lat in bb.exterior.coords], color=\"yellow\", fill=True, fill_opacity=0.3, weight=2).add_to(m)\n",
    "\n",
    "    folium.Polygon(locations=[(lat, lon) for lon, lat in hexagon.exterior.coords], color=\"blue\", fill=True, fill_opacity=0.3, weight=2).add_to(m)\n",
    "\n",
    "    if intersection:\n",
    "        folium.Polygon(locations=[(lat, lon) for lon, lat in intersection.exterior.coords],\n",
    "                       color=\"green\", fill=True, fill_opacity=0.3, weight=2).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# ID = \"4482873 5924711\"\n",
    "osm_id = '108296'\n",
    "GEORELATION_DIR = fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\llama-3.3-70b-instruct\\20250206\"\n",
    "georelated_article = load_json_file(GEORELATION_DIR, osm_id)\n",
    "\n",
    "georelation_prediction = georelated_article['georelation']\n",
    "\n",
    "h3_index = h3.latlng_to_cell(\n",
    "    georelation_prediction[\"coordinates of geographical unit\"][\"latitude\"],\n",
    "    georelation_prediction[\"coordinates of geographical unit\"][\"longitude\"],\n",
    "    get_h3_resolution_scaled(georelation_prediction[\"area in square km\"])\n",
    ")\n",
    "\n",
    "hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "hexagon_coords = [(lon, lat) for lat, lon in hexagon]\n",
    "hexagon_polygon = Polygon(hexagon_coords)\n",
    "predicted_hexagon_area = get_area_sq_km(hexagon_polygon)\n",
    "print(f\"Predicted Hexagon area: {predicted_hexagon_area} square km\")\n",
    "\n",
    "bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = get_bounding_box(osm_id)\n",
    "if bb_min_lat is not None and bb_max_lat is not None and bb_min_lon is not None and bb_max_lon is not None:\n",
    "    bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                    (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "    area = get_area_sq_km(bounding_box_polygon)\n",
    "    print(f\"Bounding box area: {area} square km\")\n",
    "\n",
    "    intersection = bounding_box_polygon.intersection(hexagon_polygon)\n",
    "\n",
    "    if not intersection.is_empty:\n",
    "        intersection_area = get_area_sq_km(intersection)\n",
    "        print(f\"Intersection area: {intersection_area} square km\")\n",
    "\n",
    "    m = create_map(bounding_box_polygon, hexagon_polygon, intersection)\n",
    "else:\n",
    "    print(\"Could not retrieve bounding box\")\n",
    "\n",
    "m"
   ],
   "id": "f2c0a081168d5747",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(evaluation_results)",
   "id": "92f87382a956b740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GeoRelating",
   "id": "4c0e917132eab8c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner') # initialize neural pipeline"
   ],
   "id": "114864b2aca49051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# article_name = \"ahrtal_floods\"\n",
    "# article_text = \"At least 19 people have died and dozens are reported missing as record rainfall in western Germany caused rivers to burst their banks, swept away homes and inundated cellars. Police said four people had died and 70 were missing on Thursday around the wine-growing hub of Ahrweiler, in Rhineland-Palatinate state, after the Ahr River that flows into the Rhine burst its banks and brought down half a dozen houses. “We have never seen such a catastrophe, it is truly devastating,” Rhineland-Palatinate premier Malu Dreyer told state lawmakers. To the north, in North Rhine-Westphalia, two people were found dead in flooded cellars in Cologne, with further deaths in Solingen, Unna and Rheinbach, police said. On Wednesday night, two firefighters died in the state – one drowned while the other collapsed after a rescue mission. About 50 people were stranded on roofs in Ahrweiler, and more houses were at risk of collapse. Police helicopters flew in from neighbouring states to winch people to safety, Koblenz police said. Weather experts said rains in the region over the past 24 hours had been unprecedented, as a near-stationary low-pressure weather system caused sustained local precipitation also to the west in France and the Netherlands. More heavy rainfall is due in southwestern Germany, on the upper reaches of the German Rhine, later on Thursday and Friday, the German Weather Service said.\"\n",
    "# article_title = \"Death toll rises in ‘devastating’ German floods\"\n",
    "# #https://www.aljazeera.com/gallery/2021/7/15/death-toll-rises-in-devastating-german-floods\n",
    "\n",
    "# article_name = \"central_europe_floods\"\n",
    "# article_text = \"Residents in several regions of Poland and the Czech Republic on Monday evacuated their homes as central Europe began recovering from the worst flooding in over 20 years, which has resulted in widespread damage and an increasing death toll. The border areas between Poland and the Czech Republic were heavily impacted over the weekend by torrential rains that began last week. Rising water levels led to the collapse of bridges, forced evacuations, and significant damage to vehicles and homes. Recent reports indicate that at least 17 fatalities have occurred due to the flooding stretching from Romania to Poland. On Monday afternoon, the mayor of Nysa, a town with over 40,000 residents in southern Poland, urged locals to evacuate immediately after a nearby floodbank was damaged. Industrial, residential areas hit hard In the northeastern Czech city of Ostrava, a breached barrier on the Odra River, where it meets the Opava River, resulted in flooding that affected the city’s industrial zone, including the BorsodChem chemical plant and the OKK Koksovny coking plant. Many residents were also being evacuated from nearby residential neighbourhoods. Meanwhile, in the Czech town of Litovel, where around 70 per cent of the area was submerged by water up to one metre deep (3.2 feet) on Monday, residents reported a swift and terrifying rise in water levels over the weekend.\"\n",
    "# article_title = \"Central Europe floods: Poland, Czech Republic evacuate amid 17 deaths\"\n",
    "# https://www.business-standard.com/world-news/central-europe-floods-poland-czech-republic-evacuate-amid-17-deaths-124091700590_1.html\n",
    "\n",
    "# article_name = \"zyklon_alfred\"\n",
    "# article_text = \"In Australien erwarten Millionen Menschen besorgt die Ankunft von Zyklon „Alfred“. Der Tropensturm bewegt sich weiter langsam, aber sicher auf eine dicht besiedelte und auch bei Touristen beliebte Region an der Ostküste zu. Jüngsten Berechnungen von Meteorologen zufolge wird er wahrscheinlich am Samstagmorgen (Ortszeit) auf Land treffen - später als zunächst erwartet. Zuletzt befand sich „Alfred“ noch etwa 100 Kilometer südöstlich der Metropole Brisbane, mit rund 2,5 Millionen Einwohnern die drittgrößte Stadt des Landes. Es ist der erste Zyklon seit 50 Jahren, der in der Region die Küste erreichen wird. Die meisten Schulen, Supermärkte, Straßen und Flughäfen wurden vorsorglich geschlossen. Die Behörden hatten Hunderttausende Sandsäcke verteilt, damit die Menschen ihre Häuser und Geschäfte sichern konnten. Auch für Tiere wie Pferde und größeres Vieh wurden Evakuierungszentren eingerichtet. Bereits die ersten Ausläufer des Wirbelsturms rissen Bäume und Strommasten um. Mehr als 80.000 Anwohner sind schon jetzt ohne Strom, wie der Sender ABC berichtete. Ein Mann wurde in der Nähe der Kleinstadt Dorrigo von den Wassermassen in einen Fluss gerissen - die Einsatzkräfte suchten nach ihm. Neun Meter hohe Wellen Vor der Küste türmten sich rund neun Meter hohe Wellen, wie Jane Golding vom staatlichen Wetterdienst betonte. Die Brandung spülte viele der bekannten Strände fort. „Wir erleben bereits Erosion an den Stränden der Gold Coast, die in dicht besiedelten Gebieten von Surfers Paradise und Main Beach bis hinunter nach Burleigh und Coolangatta Klippen von bis zu zwei Metern Höhe verursacht hat“, sagte Natalie Edwards von Surf Life Saving Queensland. Zyklon „Alfred“ nähert sich Australiens Ostküste. Starkregen, Böen von bis zu 150 Kilometern pro Stunde und Monsterwellen werden vor allem in der Region zwischen der Sunshine Coast in Queensland und dem 300 Kilometer weiter südlich gelegenen Surfer-Hotspot Byron Bay in New South Wales erwartet. Die Behörden befürchten schwere Überschwemmungen. „Heftige bis örtlich extreme Regenfälle können zu gefährlichen und lebensgefährlichen Sturzfluten führen“, warnte der Wetterdienst. „Ich rate allen, die in der Evakuierungszone leben oder sich dort aufhalten, dringend, das Gebiet jetzt zu verlassen“, sagte Polizeisprecher Scott Tanner. „Das Wetter wird sich in den nächsten Stunden verschlechtern, und dann könnte es zu spät sein, um sich in Sicherheit zu bringen.“\"\n",
    "# article_title = \"Zyklon „Alfred“ nähert sich Australiens Ostküste\"\n",
    "#https://www.faz.net/aktuell/gesellschaft/ungluecke/zyklon-alfred-naehert-sich-australiens-ostkueste-110341897.html\n",
    "\n",
    "article_name = \"the_mountain_wildfire\"\n",
    "article_text = \"A fast-moving wildfire erupted in Southern California on Wednesday, destroying homes and sending firefighters rushing to get residents out of homes and to safety, officials said. \\nThe Mountain Fire in Ventura County prompted evacuation orders and grew to over 14,000 acres, fueled by what fire officials called a significant Santa Ana wind event. \\nFirefighters at the scene of the brush fire, which broke out between the communities of Moorpark and Somis, “were faced with a tough firefight,” Ventura County Fire Capt. Trevor Johnson said. \\n“Firefighters were right off the bat engaged in pulling people out of their houses and saving lives,” Johnson said. \\nThe fire was moving so fast that firefighters drove residents out of the area in fire engines because of the danger, he said. \\nThe fire department did not have a count for the number of destroyed structures. The fire began at 8:51 a.m. local time, said the California Department of Forestry and Fire Protection, known as Cal Fire. It was 0% contained Wednesday afternoon, and what sparked it was under investigation.\"\n",
    "article_title = \"California wildfire fueled by high winds grows to over 14,000 acres and forces evacuations\"\n",
    "#https://www.nbcnews.com/news/us-news/california-mountain-fire-fueled-high-winds-evacuat-rcna179047\n",
    "\n",
    "# article_name = \"earthquakes_santorini\"\n",
    "# article_text = \"Die Erdbebenserie rund um die beliebte Ferieninsel Santorini dauert an. Die Menschen, die auf der Insel geblieben sind, wurden am Wochenende immer wieder in Angst versetzt und nachts aus dem Schlaf gerissen. Nach Angaben der Behörden hat die Erde seit dem 1. Februar Hunderte Male gebebt. Die Frequenz der Beben hat aber etwas nachgelassen. Die Seismologen geben aber keine Entwarnung. Es könnte doch noch ein größeres Beben geben, heißt es immer wieder seitens der Experten.\\nSchulen bleiben geschlossen – viele Einwohner geflohen\\nDie Regierung hat beschlossen, dass die Schulen auf Santorini und den benachbarten Eilanden Ios, Anafi und Amorgos auch kommende Woche geschlossen bleiben. \\nDer größte Teil der Bevölkerung - vor allem Frauen, Kinder und ältere Menschen - hat die Insel verlassen und sich auf dem Festland in Sicherheit gebracht. \\nAuf den Straßen trifft man Feuerwehrleute, Beamte des Zivilschutzes und der Polizei an, die aus anderen Regionen des Landes nach Santorini und den umliegenden Inseln gebracht worden sind - für den Fall, dass es zu einem schweren Erdbeben kommt. \\nDie Stadt Athen hat zahlreiche Holzhäuser in einem Sommerlager für die Menschen geöffnet, die kein Geld oder Verwandte haben, um eine neue Bleibe zu finden, bis das Phänomen um Santorini endet. \\nGefahr durch Vulkanaktivität?\\nUnklar bleibt weiterhin, inwiefern die Erdbebenserie, die sich hauptsächlich zwischen den Inseln Santorini und Amorgos ereignet, die Vulkane der Region „geweckt“ haben könnte - eine Frage, die die Einwohner besorgt. \\n1950 hatte eine Eruption des Vulkans von Santorini schwere Schäden angerichtet. Die Meinungen der Geologen und Seismologen gehen auseinander. Einige Experten schließen eine Eruption aus, andere halten sie für möglich.\\nAusharrende geben sich gelassen\\nViele Einwohner versuchen, sich Mut zu machen. Ein Mann aus Santorini sagte der Athener Zeitung „Kathimerini“ (Sonntag): „Santoriner zu sein bedeutet, auf einer Bombe zu leben und sich nicht darum zu kümmern.“\"\n",
    "# article_title = \"Kein Ende der Beben um Santorini\"\n",
    "# dpa:250209-930-369654/1"
   ],
   "id": "f7c06af770d0bccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "doc = nlp(article_text) # run annotation over a sentence\n",
    "print(doc.entities)"
   ],
   "id": "5c40cb5300153225",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "toponym_list = [entity.text for entity in doc.entities if entity.type in [\"LOC\", \"GPE\"]]",
   "id": "b27c46fea89b3a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Initialize GeoCoder\n",
    "\"\"\"\n",
    "from helpers.helpers import preprocess_data\n",
    "from models.candidates import CandidateGenerationState, GeoCodingState\n",
    "import time\n",
    "from modules.reflective_geocoding import ReflectiveGeoCoder\n",
    "\n",
    "call_times = []\n",
    "actor = \"llama-3.3-70b-instruct\"  # [\"deepseek-r1-distill-llama-70b\", \"meta-llama-3.1-8b-instruct\", \"llama-3.3-70b-instruct\", \"mistral-large-instruct\"]\n",
    "critic = \"mistral-large-instruct\"\n",
    "dataset = \"New\"\n",
    "\n",
    "geocoder = ReflectiveGeoCoder(\n",
    "    actor_model_name=actor,\n",
    "    critic_model_name=critic,\n",
    "    call_times=call_times,\n",
    "    skip_few_shot_loader=False,\n",
    "    data_set=dataset\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Reflective Candidate Generation\n",
    "\"\"\"\n",
    "generation_graph_builder = geocoder.build_graph()\n",
    "generation_agent_graph = generation_graph_builder.compile()"
   ],
   "id": "1229393a13c5351f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "input_state = {\n",
    "        \"article_id\": \"002\",\n",
    "        \"article_title\": article_title,\n",
    "        \"article_text\": article_text,\n",
    "        \"toponyms\": toponym_list\n",
    "}\n",
    "\n",
    "# Invoke the graph\n",
    "generation_agent_graph_answer = generation_agent_graph.invoke(input_state)\n",
    "candidate_generation = CandidateGenerationState(**generation_agent_graph_answer)\n",
    "print(f\"generation with {actor}_with_{critic}_critic : time taken: {time.time() - start} seconds.\")"
   ],
   "id": "bc4bed9300a53f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Run reflective candidate resolution graph\n",
    "\"\"\"\n",
    "candidate_generation_dict = preprocess_data(candidate_generation.model_dump(), GeoCodingState)\n",
    "resolution_graph_builder = geocoder.build_resolution_graph()\n",
    "resolution_agent_graph = resolution_graph_builder.compile()\n",
    "start = time.time()\n",
    "resolution_agent_graph_answer = resolution_agent_graph.invoke(candidate_generation_dict)\n",
    "candidate_resolution = GeoCodingState(**resolution_agent_graph_answer)\n",
    "print(f\"resolution with {actor}_with_{critic}_critic for: time taken: {time.time() - start} seconds.\\n\")"
   ],
   "id": "fb6b29bc6dcefcd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "GeoRelation\n",
    "\"\"\"\n",
    "from models.candidates import GeoCodingState\n",
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\georelation\\georelating\\the_mountain_fire_geocoding.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    candidate_resolution = json.load(f)\n",
    "    candidate_resolution = GeoCodingState(**candidate_resolution)\n",
    "geocoded_toponyms = []\n",
    "for topo in candidate_resolution.valid_geocoded_toponyms:\n",
    "    for item in candidate_resolution.toponyms_with_candidates:\n",
    "        if item.toponym_with_search_arguments.toponym.casefold() == topo.toponym.casefold():\n",
    "            # get the candidate with the correct geonameid\n",
    "            incorrect_geonameid = True\n",
    "            for candidate in item.candidates:\n",
    "                if topo.selected_candidate_geonameId == candidate[\"geonameId\"]:\n",
    "                    topo.coordinates = {\"latitude\": candidate[\"lat\"],\n",
    "                                        \"longitude\": candidate[\"lng\"]}\n",
    "                    geocoded_toponyms.append(topo)\n",
    "                    break\n",
    "            break\n",
    "geocoded_toponyms = [toponym.model_dump() for toponym in geocoded_toponyms]"
   ],
   "id": "150eaf5e24902d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "thoughts, prediction = georelate(\"deepseek-r1-distill-llama-70b\", geocoder.working_memory.long_term_memory, article_text, geocoded_toponyms)",
   "id": "a5826b29dc82b1b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\georelation\\georelating\\the_mountain_fire_georelation_llama_distill_R1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    output = json.load(f)\n",
    "thoughts = output['georelation_thoughts']\n",
    "prediction = output['georelation']\n",
    "print(thoughts)\n",
    "print(json.dumps(prediction, indent = 4))"
   ],
   "id": "af2027a20268c532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'output' in prediction:\n",
    "    prediction = prediction['output']\n",
    "prediction_to_save = candidate_resolution.model_dump()\n",
    "prediction_to_save[\"georelation\"] = prediction\n",
    "prediction_to_save[\"georelation_thoughts\"] = thoughts\n",
    "with open(f\"{article_name}_llama.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(prediction_to_save, f, ensure_ascii=False, indent=4, cls=CustomJSONEncoder)"
   ],
   "id": "2297dd75db36d680",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "h3_res = get_h3_resolution_scaled(prediction[\"area in square km\"])\n",
    "h3_index = h3.latlng_to_cell(\n",
    "    prediction[\"coordinates of geographical unit\"][\"latitude\"],\n",
    "    prediction[\"coordinates of geographical unit\"][\"longitude\"],\n",
    "    h3_res\n",
    ")\n",
    "\n",
    "hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "hexagon_coords = [(lon, lat) for lat, lon in hexagon]\n",
    "hexagon_polygon = Polygon(hexagon_coords)\n",
    "predicted_hexagon_area = get_area_sq_km(hexagon_polygon)\n",
    "lat_center, lon_center = (prediction[\"coordinates of geographical unit\"][\"latitude\"], prediction[\"coordinates of geographical unit\"][\"longitude\"])\n",
    "hex_popup_text = f\"Selected H3 cell: <b>{h3_index}</b><br>Predicted area: <b>{prediction[\"area in square km\"]} km²</b>\"\n",
    "hex_popup_html = f'<div style=\"font-size: 30px;\">{hex_popup_text}</div>'\n",
    "hex_popup = folium.Popup(\n",
    "    hex_popup_html,\n",
    "    max_width=450,\n",
    "    show=True,\n",
    "    sticky=True,\n",
    "    style={\"font-size\": \"30px\", \"color\": \"blue\"}\n",
    ")\n",
    "m = folium.Map(location=(lat_center, lon_center), zoom_start=10)\n",
    "folium.Polygon(locations=[(lat, lon) for lon, lat in hexagon_polygon.exterior.coords], color=\"red\", fill=True, fill_opacity=0.3, weight=2, popup=hex_popup, tooltip=h3_index).add_to(m)\n",
    "for loc in geocoded_toponyms:\n",
    "    popup = folium.Popup(\n",
    "        f'<div style=\"font-size: 24px;\"><b>{loc['toponym']}</b><br>GeoNames ID: {loc['selected_candidate_geonameId']}</div>',\n",
    "        max_width=350,\n",
    "        show=True,\n",
    "        sticky=True\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=[loc['coordinates']['latitude'], loc['coordinates']['longitude']],\n",
    "        popup=popup,\n",
    "        tooltip=loc['toponym']\n",
    "    ).add_to(m)\n",
    "m.save(f\"{article_name}_distill.html\")\n",
    "m"
   ],
   "id": "72ab1597a3c85d01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "import os\n",
    "import folium\n",
    "import json\n",
    "\n",
    "base_url = \"http://api.geonames.org/search?\"\n",
    "\n",
    "params = {\"name_equals\": \"Moorpark\",\n",
    "          \"username\": \"KaiMo\",\n",
    "          \"type\": \"json\",\n",
    "          \"style\": \"full\"}\n",
    "url = base_url + urllib.parse.urlencode(params)\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    response = requests.get(url) #retry once\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error in GeoNamesAPI.search: {response.text}\")\n",
    "all_moorparks = response.json()\n",
    "moorparks_map = folium.Map()\n",
    "for loc in all_moorparks['geonames']:\n",
    "    folium.Marker(\n",
    "        location=[loc['lat'], loc['lng']],\n",
    "        popup=loc['name'],\n",
    "        tooltip=loc['name']\n",
    "    ).add_to(moorparks_map)\n",
    "moorparks_map.save(\"all_moorparks_map.html\")\n",
    "print(json.dumps(all_moorparks, indent=4))\n",
    "moorparks_map"
   ],
   "id": "952b5e808c024e43",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
